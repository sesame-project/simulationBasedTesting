[%
	var mrs = MRS!MRS.all().first();
	var launchFileLocation = mrs.launchFileLocation;
	var simulator = mrs.simulator;
	var topicsNodesPublish = MRS!Node.all().publisher.flatten();
	topicsNodesPublish.println();
	"hi".println();
%]
import java.util.HashMap;
import java.util.Properties;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.flink.util.Collector;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;

import uk.ac.york.sesame.testing.architecture.attacks.*;
import uk.ac.york.sesame.testing.architecture.ros.ROSSimulator;
import uk.ac.york.sesame.testing.architecture.config.ConnectionProperties;
import uk.ac.york.sesame.testing.architecture.data.DataStreamManager;
import uk.ac.york.sesame.testing.architecture.data.EventMessage;
import uk.ac.york.sesame.testing.architecture.data.EventMessageSchema;

public class [%=test.name.firstToUpperCase()%]TestingTestSuiteRunner {

	public static void main(String[] args) {
		
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		Properties properties = new Properties();
		properties.setProperty("bootstrap.servers", "localhost:9092");
		properties.setProperty("group.id", "test");
		
		DataStream<EventMessage> stream = env
			.addSource(new FlinkKafkaConsumer<EventMessage>("IN", new EventMessageSchema(), properties)).returns(EventMessage.class);
		
		DataStream<EventMessage> streamOut = env
				.addSource(new FlinkKafkaConsumer<EventMessage>("OUT", new EventMessageSchema(), properties))
				.returns(EventMessage.class);
		
		// Kafka Sink to OUT
		FlinkKafkaProducer<EventMessage> myProducer = new FlinkKafkaProducer<EventMessage>("OUT", // target topic
				new EventMessageSchema(), properties);

		stream.
		[%
			for (attack in test.attacks) {
				var flatMapString = produceFlatMapSignature(attack);
				%]
				[%=flatMapString%]
				[%
			}
		%]
		addSink(myProducer);
		
//		stream.addSink(myProducer);
	
		[%
		if (simulator.isTypeOf(ROSSimulator)) { %]
		ROSSimulator rosSim = new ROSSimulator();
		ConnectionProperties cp = new ConnectionProperties();
		HashMap<String, Object> propsMap = new HashMap<String, Object>();
		propsMap.put(ConnectionProperties.HOSTNAME, "[%=simulator.hostname%]");
		propsMap.put(ConnectionProperties.PORT, [%=simulator.port%]);
		cp.setProperties(propsMap);
		
		// JRH: moved the simulation launcher outside of the thread to the main code
		HashMap<String, String> params = new HashMap<String,String>();
		params.put("launchPath", "[%=launchFileLocation%]");
		System.out.println("Simulator Starts");
		rosSim.run(params);
		rosSim.connect(cp);
		
		[%
		for (aTopic in topicsNodesPublish) { %]
		Thread subscriber_thread_[%=aTopic.name.replace("/","_")%] = new Thread() {
			public void run() {
				System.out.println("Subscriber [%=aTopic.name.replace("/","_")%] Starts");
				rosSim.consumeFromTopic("[%=aTopic.name%]", "[%=aTopic.type.name %]", true, "IN");
			}
		};
		subscriber_thread_[%=aTopic.name.replace("/","_")%].start();
		
		[%
		}
		%]
		Thread from_out_to_sim = new Thread() {
			public void run() {
				System.out.println("From out to sim starts");
				while (true) {
//					ConsumerRecords<Long, EventMessage> cr = DataStreamManager.getInstance()
//							.consume("/turtle1/cmd_vel1");
					ConsumerRecords<Long, EventMessage> cr = DataStreamManager.getInstance()
							.consume("OUT");
					for (ConsumerRecord<Long, EventMessage> record : cr) {
						System.out.println("Topic: " + record.value().getTopic().toString());
						rosSim.publishToTopic(record.value().getTopic().toString().replace(".", "/") + "OUT", record.value().getType(), record.value().getValue().toString());
					}
				}
			}
		};
		from_out_to_sim.start();
		
		// Metrics (not as stream) here
		[%
		for (aMetric in Testing!Metric.all().select(m|m.asStream == false)) { %]
		streamOut.flatMap(new [%=aMetric.name%]());
		[%
		}
		%]
		
		// Metrics (as stream) here
		[%
		for (aMetric in Testing!Metric.all().select(m|m.asStream == true)) { %]
		[%=aMetric.name%] [%=aMetric.name%] = new [%=aMetric.name%]();
		[%=aMetric.name%].calculateResult(streamOut);
		[%
		}
		%]
		
		try {
			env.execute();
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		[%
		}
		%]
		
	}

}

[%

// TODO: This operation currently works for PacketLossNetworkAttack. The if statement should be extended to construct the rest of the attacks.
operation produceFlatMapSignature(attack) {
	var flatmap = "";
	if (attack.isTypeOf(Testing!PacketLossNetworkAttack)) {
		flatmap = "flatMap(new PacketLossFlatMap(\"" + attack.topicToAttack.name + "\", \"" + attack.attackTimes.first().startTime + "\", \"" + attack.attackTimes.first().endTime + "\", " + attack.frequency + ")).";
	} else if (attack.isTypeOf(Testing!BlackholeNetworkAttack)) {
		flatmap = "flatMap(new BlackHoleFlatMap(\"" + attack.topicToAttack.name + "\", \"" + attack.attackTimes.first().startTime + "\", \"" + attack.attackTimes.first().endTime + "\")).";
	}
	return flatmap;
}
%]
